{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLP part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('classic_poems.json') as f:\n",
    "    data_json = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "content = []\n",
    "labels = []\n",
    "for row in data_json:\n",
    "    cont, poet = row['content'], row['poet_id']\n",
    "    data.append([cont, poet])\n",
    "    content.append(cont)\n",
    "    labels.append(poet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2496"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пришел и грянул октябрьский гром.\n",
      "Рвал,\n",
      "   воротил,\n",
      "      раскалывал в щепки.\n",
      "И встал\n",
      "   над бывшим\n",
      "                 буржуйским добром\n",
      "новый хозяин —\n",
      "      рабочий в кепке.\n",
      "Явился новый хозяин земли.\n",
      "Взялся за руль рукой охочей.\n",
      "— Полным ходом!\n",
      "               Вперед шевели! —\n",
      "Имя ему —\n",
      "            советский рабочий.\n",
      "За всю маету стародавних лет,\n",
      "что месили рабочих\n",
      "                  в кровавое тесто, —\n",
      "пропорол рабочий\n",
      "                хозяйский жилет,\n",
      "пригвоздил\n",
      "           штыком\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "print(data[-4][0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sent):\n",
    "    \"\"\"\n",
    "    убираем все знаки кроме киррилицы цифр, ,.?!\n",
    "    \"\"\"   \n",
    "\n",
    "    reg = re.compile('[^а-яА-Я|!?,.\\n ]')\n",
    "    sent = reg.sub('', sent).lower()\n",
    "    sent = re.findall(r\"[\\w']+|[!?,.\\n]\", sent)\n",
    "    return sent\n",
    "    \n",
    "def get_c2i(counter, min_occurrence, add):\n",
    "    \"\"\" делаем словарь из Countrer\"\"\"\n",
    "\n",
    "    \n",
    "    if add:\n",
    "        w2ind = {k: i + 1 for i, (k, v) in enumerate(counter.most_common()) if v >= min_occurrence}\n",
    "        w2ind[\"PADDING_TOKEN\"] = 0\n",
    "        w2ind[\"UNKNOWN_TOKEN\"] = len(w2ind)\n",
    "        w2ind[\"START_TOKEN\"] = len(w2ind)\n",
    "        w2ind[\"END_TOKEN\"] = len(w2ind)\n",
    "    else:\n",
    "        w2ind = {k: i for i, (k, v) in enumerate(counter.most_common()) if v >= min_occurrence}\n",
    "\n",
    "    ind2w = {v: k for k, v in w2ind.items()}\n",
    "    return w2ind, ind2w\n",
    "\n",
    "\n",
    "def get_uni2idx(text, min_occurrence=3, add=True):\n",
    "    \"\"\" Словарь униграмм\"\"\"\n",
    "    \n",
    "    unigrams_counter = Counter()  \n",
    "    for t in tqdm(text):\n",
    "        unigrams_counter.update(t)\n",
    "\n",
    "    w2ind, ind2w = get_c2i(unigrams_counter, min_occurrence=min_occurrence, add=add)\n",
    "    print('length of unigram dictionary: ', len(w2ind))\n",
    "\n",
    "    return w2ind, ind2w, unigrams_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(50) as p:\n",
    "    content = p.map(clean, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2496/2496 [00:00<00:00, 36065.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unigram dictionary:  15214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uni2idx, inx2uni, _ = get_uni2idx(content, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1434.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unigram dictionary:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lab2idx, inx2lab, lab_counter = get_uni2idx([labels], 3, add=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blok': 0, 'esenin': 2, 'mayakovskij': 4, 'pushkin': 1, 'tyutchev': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNI_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_data(data, uni2idx, lab2idx, unigrams_sequence_len):\n",
    "#     \"\"\"Возвращает векторизованные данные.\"\"\"\n",
    "    \n",
    "#     word_indices = np.array([uni2idx['PADDING_TOKEN']] * unigrams_sequence_len)\n",
    "    \n",
    "#     sentence, label = data\n",
    "    \n",
    "#     sentence = clean(sentence)\n",
    "\n",
    "#     word_indices[0] = uni2idx[\"START_TOKEN\"]\n",
    "    \n",
    "#     for pos_in_padded_sentence, word in enumerate(sentence):\n",
    "        \n",
    "#         if pos_in_padded_sentence>=(unigrams_sequence_len-1):\n",
    "#             break\n",
    "            \n",
    "#         word_idx = get_token_indices(word, uni2idx, uni2idx['UNKNOWN_TOKEN'])\n",
    "#         word_indices[pos_in_padded_sentence] = word_idx\n",
    "        \n",
    "#     word_indices[pos_in_padded_sentence] = uni2idx[\"END_TOKEN\"]    \n",
    "#     dataset = [word_indices, lab2idx[label]]\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "def vectorize_data(data, uni2idx, lab2idx, unigrams_sequence_len):\n",
    "    \"\"\"Возвращает векторизованные данные.\"\"\"\n",
    "    \n",
    "    word_indices = []\n",
    "    \n",
    "    sentence, label = data\n",
    "    \n",
    "    sentence = clean(sentence)\n",
    "\n",
    "    word_indices.append(uni2idx[\"START_TOKEN\"])\n",
    "    \n",
    "    for pos_in_padded_sentence, word in enumerate(sentence):\n",
    "            \n",
    "        word_idx = get_token_indices(word, uni2idx, uni2idx['UNKNOWN_TOKEN'])\n",
    "        word_indices.append(word_idx)\n",
    "        \n",
    "    word_indices.append(uni2idx[\"END_TOKEN\"])   \n",
    "    dataset = [word_indices, lab2idx[label]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_token_indices(token, uni2idx, uni_unknown_index):\n",
    "\n",
    "    return uni2idx.get(token, uni_unknown_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Splitting the data into train and evaluate...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing...')\n",
    "\n",
    "def vectorize(x):\n",
    "    return vectorize_data(x, uni2idx, lab2idx, UNI_SEQ_LEN)\n",
    "\n",
    "with Pool(10) as p:\n",
    "    data_vectorized = p.map(vectorize, data)\n",
    "\n",
    "\n",
    "print('Splitting the data into train and evaluate...')\n",
    "X_tr, X_ev = train_test_split(\n",
    "                              data_vectorized,\n",
    "                              test_size=0.1,\n",
    "                              random_state=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187.73642030276045, 13293, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_lengths = np.array(list(map(lambda x: len(x[0]), X_tr)))\n",
    "poem_lengths.mean(), poem_lengths.max(), poem_lengths.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 100\n",
    "N_LAYERS = 2\n",
    "HID_EMB = 100\n",
    "NUM_CLASSES = len(lab2idx)\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        A simple encoder for poem.\n",
    "        \n",
    "        output two vectors, one is responsoble for the style (author), the other - for the meaning\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(len(uni2idx), EMB_DIM, padding_idx=uni2idx['PADDING_TOKEN'])\n",
    "        self.rnn = nn.GRU(EMB_DIM, HID_EMB, N_LAYERS, batch_first=True)\n",
    "        self.style = nn.Linear(HID_EMB, HID_EMB)\n",
    "        self.meaning = nn.Linear(HID_EMB, HID_EMB)\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: \n",
    "            tuple( [batch_size, seq_len, hid_size], [batch_size, seq_len, hid_size])\n",
    "        \"\"\"\n",
    "        emb = self.emb(text_ix)\n",
    "#         print('emb', emb.shape)\n",
    "        hid, _ = self.rnn(emb)\n",
    "#         print('hid', hid.shape)\n",
    "\n",
    "        return self.style(hid), self.meaning(hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Trying to distinguish between styles based on the input vector\n",
    "        \n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()            \n",
    "        self.l1 = nn.Linear(HID_EMB, HID_EMB//2)\n",
    "        self.logits = nn.Linear(HID_EMB//2, NUM_CLASSES)\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        \"\"\"\n",
    "        :param emb: [batch_size, HID_EMB]\n",
    "        :returns:  [batch_size, NUM_CLASSES]\n",
    "        \"\"\"\n",
    "        emb = emb.mean(1)\n",
    "        emb_l1 = F.relu(self.l1(emb))\n",
    "        out = self.logits(emb_l1)\n",
    "#         print('Discriminator out', out.shape)\n",
    "        return out\n",
    "\n",
    "class Motivator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Trying to distinguish between styles based on the input vector\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.m = Discriminator()\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        \"\"\"\n",
    "        :param emb: [batch_size, HID_EMB]\n",
    "        :returns: [batch_size, NUM_CLASSES]\n",
    "        \"\"\"\n",
    "        out = self.m(emb)\n",
    "#         print('Motivator out', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,):\n",
    "        \"\"\" \n",
    "        A sequential decoder.\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(2*HID_EMB, 2*HID_EMB, N_LAYERS, batch_first=True)\n",
    "        self.logits = nn.Linear(2*HID_EMB, len(uni2idx))\n",
    "        \n",
    "    def forward(self, emb_style, emb_meaning):\n",
    "        \"\"\"\n",
    "        :param text_ix: tuple( [batch_size, seq_len, hid_size], [batch_size, seq_len, hid_size])\n",
    "        :returns: [batch_size, tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        emb = torch.cat([emb_style, emb_meaning], dim=-1)\n",
    "        hid, _ = self.rnn(emb)\n",
    "        logits = self.logits(hid)\n",
    "#         print('Decoder logits', logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_criterion = nn.CrossEntropyLoss(ignore_index = uni2idx['PADDING_TOKEN'])  \n",
    "m_criterion = nn.CrossEntropyLoss() \n",
    "d_criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "def m_loss(logits, labels):\n",
    "\n",
    "    loss = m_criterion(logits, labels)\n",
    "    \n",
    "    return  loss \n",
    "\n",
    "def d_loss(logits, labels):\n",
    "      \n",
    "    loss = d_criterion(logits, labels)\n",
    "        \n",
    "    return  loss \n",
    "\n",
    "def g_loss(logits, poem):\n",
    "\n",
    "    poem_next = poem[:, 1:].contiguous()\n",
    "     \n",
    "    loss = g_criterion(logits.view(-1, len(uni2idx)) , poem_next.view(-1)) \n",
    "        \n",
    "    return  loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimazers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "discriminator = Discriminator()\n",
    "motivator = Motivator()\n",
    "generator = Generator()\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "from torch import optim\n",
    "\n",
    "g_optimizer = optim.Adam(chain(encoder.parameters(),\n",
    "                             generator.parameters()),  lr=LR, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=LR, betas=(0.5, 0.999))\n",
    "m_optimizer = optim.Adam(motivator.parameters(), lr=LR, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_batch(sequences, max_len=None, global_max=500, pad_index = uni2idx['PADDING_TOKEN']):\n",
    "    \"\"\" pad batch with max len \"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,sequences))    \n",
    "    max_len = min(global_max, max_len)\n",
    "    \n",
    "#     print(len(sequences))\n",
    "    sequences = pad_sequences(sequences, maxlen=max_len,  padding='post', value=pad_index)\n",
    "#     print(len(sequences))\n",
    "    print('sequences', len(sequences[0]))\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        poems = [data[i][0] for i in indices[start : start + batch_size]]\n",
    "        poems = pad_batch(poems)\n",
    "        labels = [data[i][1] for i in indices[start : start + batch_size]]\n",
    "        \n",
    "        yield Variable(torch.LongTensor(poems)), \\\n",
    "                       Variable(torch.LongTensor(labels))\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "n_batches_per_epoch = len(X_tr)\n",
    "n_validation_batches = len(X_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "\n",
    "def train(X_tr, X_ev, batch_size=3, n_epochs=3, verbose=True):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        X_tr, X_te, y_tr, y_te: datasets\n",
    "        batch_size: int\n",
    "    returns:\n",
    "        generator: model\n",
    "        epoch: integer (the last epoch)\n",
    "        history: dict\n",
    "    \"\"\"\n",
    "    \n",
    "    n_train_batches = math.ceil(len(X_tr) / batch_size)\n",
    "    n_validation_batches = math.ceil(len(X_ev) / batch_size)\n",
    "\n",
    "    history = {'generator_train_loss': [], 'generator_val_loss': [],\n",
    "               'motivator_train_loss': [], 'motivator_val_loss': [], 'discriminator_train_loss': [], \n",
    "               'discriminator_val_loss': [], 'overall_train_loss': [], 'overall_val_loss': []}\n",
    "    \n",
    "    dis_loss_c, mot_loss_c, gen_loss_c, overall_loss_c = 0, 0, 0, 0\n",
    "\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            discriminator.train(True); motivator.train(True); \n",
    "            generator.train(True); encoder.train(True); \n",
    "            for poems, labels in iterate_minibatches(X_tr, batch_size):\n",
    "\n",
    "                # encoding\n",
    "                poem_inp = poems[:, :-1].contiguous()\n",
    "                enc_style, enc_mean = encoder(poem_inp)  \n",
    "\n",
    "                # discriminator step\n",
    "                dis_loss = d_loss(discriminator(enc_mean), labels)\n",
    "                d_optimizer.zero_grad()\n",
    "                dis_loss.backward(retain_graph=True)\n",
    "                d_optimizer.step()\n",
    "\n",
    "                # motivator step\n",
    "                mot_loss = m_loss(motivator(enc_style), labels)\n",
    "                m_optimizer.zero_grad()\n",
    "                mot_loss.backward(retain_graph=True)\n",
    "                m_optimizer.step()\n",
    "\n",
    "                #generator step\n",
    "                gen_loss = g_loss(generator(enc_style, enc_mean), poems)\n",
    "                overall_loss = gen_loss + mot_loss - dis_loss\n",
    "                g_optimizer.zero_grad()\n",
    "                overall_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "                dis_loss_c += dis_loss.cpu().data.numpy()\n",
    "                mot_loss_c += mot_loss.cpu().data.numpy()\n",
    "                gen_loss_c += gen_loss.cpu().data.numpy()\n",
    "                overall_loss_c += overall_loss.cpu().data.numpy()\n",
    "                \n",
    "                \n",
    "            history['generator_train_loss'].append(gen_loss_c/n_train_batches)\n",
    "            history['motivator_train_loss'].append(mot_loss_c/n_train_batches)\n",
    "            history['discriminator_train_loss'].append(dis_loss_c/n_train_batches)\n",
    "            history['overall_train_loss'].append(overall_loss_c/n_train_batches)\n",
    "            \n",
    "            discriminator.train(False); motivator.train(False); \n",
    "            generator.train(False); encoder.train(False); \n",
    "            dis_loss_c, mot_loss_c, gen_loss_c, overall_loss_c = 0, 0, 0, 0\n",
    "            for poems, labels in iterate_minibatches(X_ev, batch_size):\n",
    "\n",
    "                # encoding\n",
    "                poem_inp = poems[:, :-1].contiguous()\n",
    "                enc_style, enc_mean = encoder(poem_inp)  \n",
    "\n",
    "                # discriminator step\n",
    "                dis_loss = d_loss(discriminator(enc_mean), labels)\n",
    "                # motivator step\n",
    "                mot_loss = m_loss(motivator(enc_style), labels)\n",
    "                #generator step\n",
    "                gen_loss = g_loss(generator(enc_style, enc_mean), poems)\n",
    "                overall_loss = gen_loss + mot_loss - dis_loss\n",
    "                \n",
    "                dis_loss_c += dis_loss.cpu().data.numpy()\n",
    "                mot_loss_c += mot_loss.cpu().data.numpy()\n",
    "                gen_loss_c += gen_loss.cpu().data.numpy()\n",
    "                overall_loss_c += overall_loss.cpu().data.numpy()\n",
    "                \n",
    "                \n",
    "            history['generator_val_loss'].append(gen_loss_c/n_validation_batches)\n",
    "            history['motivator_val_loss'].append(mot_loss_c/n_validation_batches)\n",
    "            history['discriminator_val_loss'].append(dis_loss_c/n_validation_batches)\n",
    "            history['overall_val_loss'].append(overall_loss_c/n_validation_batches)\n",
    "            \n",
    "            # Visualize\n",
    "            if verbose:\n",
    "                display.clear_output(wait=True)\n",
    "                plt.figure(figsize=(16, 6))\n",
    "                # Then we print the results for this epoch:\n",
    "                print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                    epoch + 1, n_epochs, time.time() - start_time))\n",
    "                print('current overall train loss: {}'.format(history['overall_train_loss'][-1]))\n",
    "                print('current overall val loss: {}'.format(history['overall_val_loss'][-1]))\n",
    "                plot_history(history)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        return  epoch, history\n",
    "\n",
    "    print(\"Finished!\")\n",
    "       \n",
    "    return  epoch, history\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.subplot(221)\n",
    "    plt.title(\"generator\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['generator_train_loss'], 'b', label='train_loss')\n",
    "    plt.plot(history['generator_val_loss'], 'g', label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(222)\n",
    "    plt.title(\"motivator\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['motivator_train_loss'], label=\"train_loss\")\n",
    "    plt.plot(history['motivator_val_loss'], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(223)\n",
    "    plt.title(\"discriminator\")\n",
    "    plt.xlabel(\"#epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['discriminator_train_loss'], label=\"train_loss\")\n",
    "    plt.plot(history['discriminator_val_loss'], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(224)\n",
    "    plt.title(\"overall\")\n",
    "    plt.xlabel(\"#epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['overall_train_loss'], label=\"train_loss\")\n",
    "    plt.plot(history['overall_val_loss'], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e025aa80684c64956cf1c8bd0606ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=281), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 230\n",
      "sequences 466\n",
      "sequences 500\n",
      "sequences 176\n",
      "sequences 283\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 275\n",
      "sequences 365\n",
      "sequences 427\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 210\n",
      "sequences 230\n",
      "sequences 279\n",
      "sequences 500\n",
      "sequences 187\n",
      "sequences 238\n",
      "sequences 235\n",
      "sequences 302\n",
      "sequences 269\n",
      "sequences 403\n",
      "sequences 344\n",
      "sequences 198\n",
      "sequences 187\n",
      "sequences 198\n",
      "sequences 100\n",
      "sequences 500\n",
      "sequences 486\n",
      "sequences 355\n",
      "sequences 205\n",
      "sequences 316\n",
      "sequences 249\n",
      "sequences 500\n",
      "sequences 284\n",
      "sequences 274\n",
      "sequences 500\n",
      "sequences 105\n",
      "sequences 226\n",
      "sequences 196\n",
      "sequences 177\n",
      "sequences 405\n",
      "sequences 372\n",
      "sequences 242\n",
      "sequences 432\n",
      "sequences 500\n",
      "sequences 264\n",
      "sequences 290\n",
      "sequences 178\n",
      "sequences 226\n",
      "sequences 315\n",
      "sequences 298\n",
      "sequences 500\n",
      "sequences 459\n",
      "sequences 220\n",
      "sequences 189\n",
      "sequences 473\n",
      "sequences 318\n",
      "sequences 500\n",
      "sequences 404\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 319\n",
      "sequences 500\n",
      "sequences 324\n",
      "sequences 366\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 277\n",
      "sequences 500\n",
      "sequences 423\n",
      "sequences 500\n",
      "sequences 468\n",
      "sequences 500\n",
      "sequences 110\n",
      "sequences 114\n",
      "sequences 500\n",
      "sequences 208\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 445\n",
      "sequences 304\n",
      "sequences 198\n",
      "sequences 500\n",
      "sequences 198\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 186\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 213\n",
      "sequences 500\n",
      "sequences 378\n",
      "sequences 500\n",
      "sequences 219\n",
      "sequences 500\n",
      "sequences 329\n",
      "sequences 172\n",
      "sequences 314\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 173\n",
      "sequences 288\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 468\n",
      "sequences 150\n",
      "sequences 328\n",
      "sequences 258\n",
      "sequences 135\n",
      "sequences 180\n",
      "sequences 237\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 217\n",
      "sequences 500\n",
      "sequences 269\n",
      "sequences 500\n",
      "sequences 442\n",
      "sequences 302\n",
      "sequences 406\n",
      "sequences 247\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 180\n",
      "sequences 500\n",
      "sequences 348\n",
      "sequences 198\n",
      "sequences 233\n",
      "sequences 358\n",
      "sequences 499\n",
      "sequences 395\n",
      "sequences 174\n",
      "sequences 357\n",
      "sequences 251\n",
      "sequences 283\n",
      "sequences 116\n",
      "sequences 393\n",
      "sequences 342\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 500\n",
      "sequences 496\n"
     ]
    }
   ],
   "source": [
    "epoch, history = train(X_tr, X_ev, batch_size=8, n_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating some masterpieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding style embeddings  for the authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_embeddings(data, size=5, max_len=100):\n",
    "    \"\"\"ищем эбеддинги стиля для каждого автора\"\"\"\n",
    "\n",
    "    embeddings = {key:[] for key in lab2idx.keys()}    \n",
    "    labels = np.array([data[i][-1] for i in range(len(data))])\n",
    "    \n",
    "    for key in embeddings.keys():\n",
    "        \n",
    "        indices = np.where(lab2idx[key]==labels)[0]      \n",
    "        indices = np.random.choice(indices, size=size) \n",
    "        \n",
    "        data_author = pad_batch([data[i][0] for i in indices], max_len=max_len)\n",
    "        poems = Variable(torch.LongTensor(data_author), volatile=True)      \n",
    "        poem_inp = poems[:, :-1].contiguous()\n",
    "        \n",
    "        emb_style, emb_meaning = encoder.forward(poem_inp)       \n",
    "        embeddings[key] = emb_style\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 101\n",
      "sequences 101\n",
      "sequences 101\n",
      "sequences 101\n",
      "sequences 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "embeddings = find_embeddings(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 100, 100])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['blok'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(prefix_ix = uni2idx[\"START_TOKEN\"], \n",
    "                     t=1, sample=True, max_len=100, author = None):\n",
    "    \n",
    "    \"\"\"генирим произведения по автору, если автор не задан - используется эмбеддинг стиля из энкодера\"\"\"\n",
    "     \n",
    "    poem_prefix = [prefix_ix]\n",
    "    phrase = []\n",
    "    for _ in range(max_len-1):\n",
    "        \n",
    "        prefix_ix = [poem_prefix]\n",
    "        \n",
    "        prefix_ix = Variable(torch.LongTensor(prefix_ix), volatile=True)\n",
    "#         print(prefix_ix.shape)\n",
    "            \n",
    "        emb_style, emb_meaning = encoder.forward(prefix_ix)\n",
    "\n",
    "        if author is not None:\n",
    "            emb_meaning = embeddings[author][:, :len(poem_prefix)].mean(0, keepdim=True)\n",
    "#             print(emb_style.shape, emb_meaning.shape)\n",
    "            next_word_logits = generator.forward(emb_style, emb_meaning)[0, -1]\n",
    "        else:    \n",
    "            next_word_logits = generator.forward(emb_style, emb_meaning)[0, -1]\n",
    "#         print(next_word_logits.shape)\n",
    "        next_word_probs = F.softmax(next_word_logits, -1).cpu().data.numpy()\n",
    "        \n",
    "        assert len(next_word_probs.shape) == 1, 'probs must be one-dimensional'\n",
    "        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n",
    "\n",
    "        if sample:\n",
    "            while True:\n",
    "                next_word_ind = np.random.choice(len(uni2idx), p=next_word_probs) \n",
    "                if next_word_ind != uni2idx['UNKNOWN_TOKEN']: # выбираем пока не выверем что-то отличное от неизвестного символа\n",
    "                    break\n",
    "        else:\n",
    "            next_word_probs[uni2idx['UNKNOWN_TOKEN']]=0 # зануляем вероятность неизвестного символа\n",
    "            next_word_ind = np.argmax(next_word_probs)\n",
    "\n",
    "        poem_prefix.append(next_word_ind)\n",
    "        \n",
    "#         print('next_word_ind', next_word_ind)\n",
    "        next_word = inx2uni[next_word_ind]\n",
    "        phrase.append(next_word)\n",
    "\n",
    "        if next_word_ind==uni2idx[\"END_TOKEN\"]:\n",
    "            break\n",
    "            \n",
    "    return ' '+' '.join(phrase[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase = generate_caption(sample=True, author = 'blok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blok: \n",
      "\n",
      " *****\n",
      " призыва , . не вс меня по а тянется прелестных сердцах\n",
      "\n",
      "pushkin: \n",
      "\n",
      " *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ветлы , что , спрашиваете закованный торопливо \n",
      " , на \n",
      " . . раб \n",
      " даль часть , , \n",
      " я алтаря своим , дубровы много . \n",
      " небо с вечерних лишь я тихо и он осенью на , \n",
      " торжество крылатых , сильней жизнь тебя знает я ? . дым опять . мне . что , за готовить \n",
      " \n",
      " и \n",
      " зеркало не и не и . \n",
      " \n",
      " . помоги дубе смерть , вы \n",
      " \n",
      " служу вечные иерусалим прошел ответа \n",
      " моих не моей звенит а . \n",
      " я ! \n",
      " ж во вызов ж\n",
      "\n",
      "esenin: \n",
      "\n",
      " *****\n",
      " минет немые , давил . отрадный дума на ! микола \n",
      " \n",
      " о , стеною которые подходила но день счастливый \n",
      " в вокруг страсти , \n",
      " напоследок сердца \n",
      " , ночь кровь \n",
      " нет езерский , меня не . друзья светел в стрелой плыл селе . с пой . станет \n",
      " спасенья ты в , , о \n",
      " тишине в в ! \n",
      " золотой звездный прочь ни свободном наш верь в душа ты брегам та ? \n",
      " нею . , \n",
      " в и \n",
      " . , \n",
      " ? , \n",
      " , . под мир не \n",
      " ночи . что\n",
      "\n",
      "tyutchev: \n",
      "\n",
      " *****\n",
      " меже вступила двери душой \n",
      " и свой с улетели заметили сердце того лет\n",
      "\n",
      "mayakovskij: \n",
      "\n",
      " *****\n",
      " обманывать \n",
      " ложе хвалой \n",
      " пищу злодей , нам . не , твоим \n",
      " . , как \n",
      " живет огни ты \n",
      " . былых высоко . божий , веселый \n",
      " замять чуть разом что стрел о его свой \n",
      " теплится не \n",
      " оврага игом альбом быть \n",
      " куста тени для создали бала нет за . , , . \n",
      " \n",
      " сердце , царила . \n",
      " . лазурного , вы \n",
      " \n",
      " , сильней . \n",
      " и счастливо \n",
      " бремя чиста \n",
      " вкруг гибель горячо христиан придут ее только . местью плену \n",
      " , \n",
      " дни таких\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for author in embeddings.keys():\n",
    "    print(author+': \\n')\n",
    "    print(' '+'*'*5)\n",
    "    phrase = generate_poem(sample=True, author = author) \n",
    "    print(phrase)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
