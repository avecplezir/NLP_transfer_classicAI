{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLP part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('classic_poems.json') as f:\n",
    "    data_json = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "content = []\n",
    "labels = []\n",
    "for row in data_json:\n",
    "    data.append([row['content'], row['poet_id']])\n",
    "    content.append(row['content'])\n",
    "    labels.append(row['poet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2496"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ярмарка.\n",
      "    Вовсю!\n",
      "        Нелепица на нелепице.\n",
      "Лейпциг гудит.\n",
      "        Суетится Лейпциг.\n",
      "Но площад\n"
     ]
    }
   ],
   "source": [
    "print(data[-45][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sent):\n",
    "    \"\"\"\n",
    "    убираем все знаки кроме киррилицы цифр, ,.?!\n",
    "    \"\"\"   \n",
    "\n",
    "    reg = re.compile('[^а-яА-Я|!?,.\\n ]')\n",
    "    sent = reg.sub('', sent).lower()\n",
    "    sent = re.findall(r\"[\\w']+|[!?,.\\n]\", sent)\n",
    "    return sent\n",
    "    \n",
    "def get_c2i(counter, min_occurrence, add):\n",
    "    \"\"\" делаем словарь из Countrer\"\"\"\n",
    "\n",
    "    w2ind = {k: i + 1 for i, (k, v) in enumerate(counter.most_common()) if v >= min_occurrence}\n",
    "    if add:\n",
    "        w2ind[\"PADDING_TOKEN\"] = 0\n",
    "        w2ind[\"UNKNOWN_TOKEN\"] = len(w2ind)\n",
    "\n",
    "    ind2w = {v: k for k, v in w2ind.items()}\n",
    "    return w2ind, ind2w\n",
    "\n",
    "\n",
    "def get_uni2idx(text, min_occurrence=3, add=True):\n",
    "    \"\"\" Словарь униграмм\"\"\"\n",
    "    \n",
    "    unigrams_counter = Counter()  \n",
    "    for t in tqdm(text):\n",
    "        unigrams_counter.update(t)\n",
    "\n",
    "    w2ind, ind2w = get_c2i(unigrams_counter, min_occurrence=min_occurrence, add=add)\n",
    "    print('length of unigram dictionary: ', len(w2ind))\n",
    "\n",
    "    return w2ind, ind2w, unigrams_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(50) as p:\n",
    "    content = p.map(clean, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2496/2496 [00:00<00:00, 29342.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unigram dictionary:  15212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "uni2idx, inx2uni, _ = get_uni2idx(content, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2079.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unigram dictionary:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lab2idx, inx2lab, lab_counter = get_uni2idx([labels], 3, add=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blok': 1, 'esenin': 3, 'mayakovskij': 5, 'pushkin': 2, 'tyutchev': 4}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNI_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(data, uni2idx, unigrams_sequence_len):\n",
    "    \"\"\"Возвращает векторизованные данные.\"\"\"\n",
    "    \n",
    "    uni_unknown_index = uni2idx['UNKNOWN_TOKEN']\n",
    "    uni_padding_index = uni2idx['PADDING_TOKEN']\n",
    "    word_indices = np.array([uni_padding_index] * unigrams_sequence_len)\n",
    "    \n",
    "    sentence, label = data\n",
    "    \n",
    "    sentence = clean(sentence)\n",
    "\n",
    "    for pos_in_padded_sentence, word in enumerate(sentence):\n",
    "        \n",
    "        if pos_in_padded_sentence>=unigrams_sequence_len:\n",
    "            break\n",
    "        word_idx = get_token_indices(word, uni2idx, uni_unknown_index)\n",
    "        word_indices[pos_in_padded_sentence] = word_idx\n",
    "        \n",
    "    dataset = [word_indices, lab2idx[label]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_token_indices(token, uni2idx, uni_unknown_index):\n",
    "\n",
    "    return uni2idx.get(token, uni_unknown_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Splitting the data into train and evaluate...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing...')\n",
    "\n",
    "def vectorize(x):\n",
    "    return vectorize_data(x, uni2idx, UNI_SEQ_LEN)\n",
    "\n",
    "with Pool(10) as p:\n",
    "    data_vectorized = p.map(vectorize, data)\n",
    "\n",
    "\n",
    "print('Splitting the data into train and evaluate...')\n",
    "X_tr, X_ev = train_test_split(\n",
    "                              data_vectorized,\n",
    "                              test_size=0.1,\n",
    "                              random_state=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 100\n",
    "N_LAYERS = 2\n",
    "HID_EMB = 100\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        A simple encoder for poem.\n",
    "        \n",
    "        output two vectors, one is responsoble for the style (author), the other - for the meaning\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(len(uni2idx), EMB_DIM, padding_idx=uni2idx['PADDING_TOKEN'])\n",
    "        self.rnn = nn.GRU(EMB_DIM, HID_EMB, N_LAYERS, batch_first=True)\n",
    "        self.style = nn.Linear(HID_EMB, HID_EMB)\n",
    "        self.meaning = nn.Linear(HID_EMB, HID_EMB)\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: \n",
    "            tuple( [batch_size, seq_len, hid_size], [batch_size, seq_len, hid_size])\n",
    "        \"\"\"\n",
    "        emb = self.emb(text_ix)\n",
    "        print('emb', emb.shape)\n",
    "        hid, _ = self.rnn(emb)\n",
    "        print('hid', hid.shape)\n",
    "\n",
    "        return self.style(hid), self.meaning(hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Trying to distinguish between styles based on the input vector\n",
    "        \n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()            \n",
    "        self.l1 = nn.Linear(HID_EMB, HID_EMB//2)\n",
    "        self.logits = nn.Linear(HID_EMB//2, NUM_CLASSES)\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        \"\"\"\n",
    "        :param emb: [batch_size, HID_EMB]\n",
    "        :returns:  [batch_size, NUM_CLASSES]\n",
    "        \"\"\"\n",
    "        emb = emb.mean(1)\n",
    "        emb_l1 = F.relu(self.l1(emb))\n",
    "        out = self.logits(emb_l1)\n",
    "        print('Discriminator out', out.shape)\n",
    "        return out\n",
    "\n",
    "class Motivator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Trying to distinguish between styles based on the input vector\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.m = Discriminator()\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param emb: [batch_size, HID_EMB]\n",
    "        :returns: [batch_size, NUM_CLASSES]\n",
    "        \"\"\"\n",
    "        out = self.m(emb)\n",
    "        print('Motivator out', out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        \"\"\" \n",
    "        A sequential decoder.\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(HID_EMB, 2*HID_EMB, N_LAYERS, batch_first=True)\n",
    "        self.logits = nn.Linear(2*HID_EMB, len(uni2idx))\n",
    "        \n",
    "    def forward(self, emb_style, emb_meaning):\n",
    "        \"\"\"\n",
    "        :param text_ix: tuple( [batch_size, seq_len, hid_size], [batch_size, seq_len, hid_size])\n",
    "        :returns: [batch_size, tokens]\n",
    "        \"\"\"\n",
    "        \n",
    "        emb = torch.cat([emb_style, emb_meaning], dim=-1)\n",
    "        hid = self.rnn(emb)\n",
    "        logits = self.logits(hid)\n",
    "        print('Decoder logits', logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_loss(False_data, mode):\n",
    "    if mode == 1: \n",
    "        loss = (1-False_data).log().mean()\n",
    "    if mode == 2:\n",
    "        loss = -False_data.log().mean()\n",
    "    \n",
    "    if mode == 3 or TASK == 4:\n",
    "        loss = -False_data.mean()\n",
    "    \n",
    "    return  loss \n",
    "\n",
    "def d_loss(False_data, True_data, mode):\n",
    "    if mode == 1 or mode == 2: \n",
    "        loss = -(True_data.log().mean(0)+(1-False_data).log().mean())\n",
    "        \n",
    "    if mode == 3 or mode == 4:\n",
    "        loss = -(-False_data.mean()+True_data.mean())\n",
    "        \n",
    "    return  loss \n",
    "\n",
    "def decoder_loss(logits, labels):\n",
    "    \n",
    "    loss = -(True_data.log().mean(0)+(1-False_data).log().mean())\n",
    "        \n",
    "    return  loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tnrange\n",
    "def iterate_minibatches(data, batch_size=32, shuffle=True, verbose=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    \n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield [data[i] for i in indices[start : start + batch_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "n_batches_per_epoch = len(train_data)\n",
    "n_validation_batches = len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.subplot(131)\n",
    "    plt.title(\"decoder\")\n",
    "    plt.xlabel(\"#epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['decoder_train_loss'], 'b', label='train_loss')\n",
    "    plt.plot(history['decoder_val_loss'], 'g', label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(132)\n",
    "    plt.title(\"motivator\")\n",
    "    plt.xlabel(\"#epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['motivator_train_loss'], label=\"train_loss\")\n",
    "    plt.plot(history['motivator_val_loss'], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.subplot(133)\n",
    "    plt.title(\"discriminator\")\n",
    "    plt.xlabel(\"#epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(history['discriminator_train_loss'], label=\"train_loss\")\n",
    "    plt.plot(history['discriminator_val_loss'], label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(generator, X_tr, X_te, y_tr, y_te, batchsize=3, n_epochs=3, verbose=True):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        generator: model\n",
    "        X_tr, X_te, y_tr, y_te: datasets\n",
    "        batchsize: int\n",
    "    returns:\n",
    "        generator: model\n",
    "        epoch: integer (the last epoch)\n",
    "        history: dict\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = optim.Adam(generator.parameters())\n",
    "    n_train_batches = math.ceil(len(X_tr) / batchsize)\n",
    "    n_validation_batches = math.ceil(len(X_te) / batchsize)\n",
    "\n",
    "    history = {'decoder_train_loss': [], 'decoder_val_loss': [],\n",
    "               'motivator_train_loss': [], 'motivator_val_loss': [], 'discriminator_train_loss': [], \n",
    "               'discriminator_val_loss': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = 0\n",
    "        generator.train(True)\n",
    "        \n",
    "        try:\n",
    "            for X, y in tqdm(iterate_minibatches(X_tr, y_tr, batchsize)):\n",
    "                pred, sound = generator(X, y)\n",
    "                loss = compute_loss(pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.cpu().data.numpy()\n",
    "\n",
    "            train_loss /= n_train_batches\n",
    "\n",
    "            generator.train(False)\n",
    "            val_loss = 0\n",
    "            for X, y in tqdm(iterate_minibatches(X_te, y_te, batchsize)):\n",
    "                pred, sound = generator(X, y)\n",
    "                loss = compute_loss(pred, y)\n",
    "\n",
    "                val_loss += loss.cpu().data.numpy()\n",
    "\n",
    "            val_loss /= n_validation_batches\n",
    "\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            # metrics computed on the last val batch\n",
    "            history['EB_true'].append(EB(y).mean().data.cpu().numpy())\n",
    "            history['UPC_true'].append(UPC(y).mean().data.cpu().numpy())\n",
    "            history['QN_true'].append(QN(y).mean().data.cpu().numpy())\n",
    "            history['EB_false'].append(EB(sound).mean().data.cpu().numpy())\n",
    "            history['UPC_false'].append(UPC(sound).mean().data.cpu().numpy())\n",
    "            history['QN_false'].append(QN(sound).mean().data.cpu().numpy())\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            return generator, epoch, history\n",
    "\n",
    "            # Visualize\n",
    "        if verbose:\n",
    "            display.clear_output(wait=True)\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, n_epochs, time.time() - start_time))\n",
    "            print('current train loss: {}'.format(history['train_loss'][-1]))\n",
    "            print('current val loss: {}'.format(history['val_loss'][-1]))\n",
    "            plot_history(history)\n",
    "\n",
    "    print(\"Finished!\")\n",
    "\n",
    "    return generator, epoch, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss=0\n",
    "    question_vectorizer.train(True)\n",
    "    answer_vectorizer.train(True)\n",
    "    it = iterate_minibatches(train_data, verbose = False)\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        \n",
    "        batch = next(it)\n",
    "        loss = compute_loss(*batch)     \n",
    "        \n",
    "        # clear old gradients; do a backward pass to get new gradients; then train with opt\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "#         print('loss', loss)\n",
    "        train_loss += loss.cpu().data.numpy()\n",
    "        recall = compute_recall(*batch)\n",
    "        train_recall += recall\n",
    "        \n",
    "    train_loss /= n_batches_per_epoch\n",
    "    train_recall /=n_batches_per_epoch\n",
    "    \n",
    "    val_loss=0\n",
    "    question_vectorizer.train(False)\n",
    "    answer_vectorizer.train(False)\n",
    "    it = iterate_minibatches(val_data, verbose = False)\n",
    "    for _ in range(n_validation_batches):\n",
    "        batch = next(it)\n",
    "        loss = compute_loss(*batch)\n",
    "        val_loss += loss.cpu().data.numpy()\n",
    "        recall = compute_recall(*batch)\n",
    "        val_recall += recall\n",
    "        \n",
    "        \n",
    "    val_loss /= n_validation_batches\n",
    "    val_recall /=n_validation_batches\n",
    "    \n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "    print('\\nEpoch: {}, train recall: {}, val recall: {}'.format(epoch, train_recall, val_recall))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating some masterpieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
